{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Hanoi, Vietnam housing price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from scikeras.wrappers import KerasRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing libraries, cleaning data, and removing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignoring future warnings and deprecation warnings so as not to make the notebook full of warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the first few lines of the dataset\n",
    "df = pd.read_csv(\"./data.csv\")\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The length of the dataset before cleaning and removing outliers\n",
    "print(\"The initial length of the dataset is\", str(len(df)), \"rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data and removing outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renaming/Translating column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_renamed = df.rename(columns = {\"Ngày\":\"date\", \"Địa chỉ\":\"address\", \"Quận\":\"district\", \n",
    "                                  \"Huyện\":\"ward\", \"Loại hình nhà ở\":\"type_of_housing\",\n",
    "                                 \"Giấy tờ pháp lý\":\"legal_paper\", \"Số tầng\":\"num_floors\",\n",
    "                                 \"Số phòng ngủ\":\"num_bed_rooms\", \"Diện tích\":\"squared_meter_area\",\n",
    "                                 \"Dài\":\"length_meter\", \"Rộng\":\"width_meter\", \"Giá/m2\":\"price_in_million_per_square_meter\"})\n",
    "df_renamed = df_renamed.drop(\"Unnamed: 0\", axis = 1)\n",
    "df_renamed = df_renamed.dropna()\n",
    "df_renamed = df_renamed.reset_index()\n",
    "\n",
    "# The length of the dataset after dropping null values\n",
    "print(\"The length of the dataset after dropping null values is\", str(len(df_renamed)), \"rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning data within each columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove houses with \"10 plus\" floors and bed rooms, since this cannot be exactly quantified\n",
    "df_renamed = df_renamed[df_renamed['num_floors'] != 'Nhiều hơn 10']\n",
    "df_renamed = df_renamed[df_renamed['num_bed_rooms'] != 'nhiều hơn 10 phòng']\n",
    "\n",
    "# Clean columns and convert numerical columns to float type\n",
    "df_renamed['district'] = df_renamed['district'].str.replace('Quận ','').str.strip()\n",
    "df_renamed['ward'] = df_renamed['ward'].str.replace('Phường ','').str.strip()\n",
    "df_renamed['num_floors'] = df_renamed['num_floors'].str.strip().astype(float)\n",
    "df_renamed['num_bed_rooms'] = df_renamed['num_bed_rooms'].str.replace(' phòng','').str.strip().astype(float)\n",
    "df_renamed['squared_meter_area'] = df_renamed['squared_meter_area'].str.replace(' m²','').str.strip().astype(float)\n",
    "df_renamed['length_meter'] = df_renamed['length_meter'].str.replace(' m','').str.strip().astype(float)\n",
    "df_renamed['width_meter'] = df_renamed['width_meter'].str.replace(' m','').str.strip().astype(float)\n",
    "\n",
    "# Clean and convert all prices to million/m2 instead of VND/m2 or billion/m2\n",
    "df_renamed.loc[df_renamed['price_in_million_per_square_meter'].str.contains(' tỷ/m²'), 'price_in_million_per_square_meter'] = df_renamed.loc[df_renamed['price_in_million_per_square_meter'].str.contains(' tỷ/m²'), 'price_in_million_per_square_meter'].str.replace(' tỷ/m²','').str.replace('.','').str.replace(',','.').astype(float) * 1000\n",
    "df_renamed.loc[df_renamed['price_in_million_per_square_meter'].str.contains(' triệu/m²', na=False), 'price_in_million_per_square_meter'] = df_renamed.loc[df_renamed['price_in_million_per_square_meter'].str.contains(' triệu/m²', na=False), 'price_in_million_per_square_meter'].str.replace(' triệu/m²','').str.replace(',','.').astype(float)\n",
    "df_renamed.loc[df_renamed['price_in_million_per_square_meter'].str.contains(' đ/m²', na=False), 'price_in_million_per_square_meter'] = df_renamed.loc[df_renamed['price_in_million_per_square_meter'].str.contains(' đ/m²', na=False), 'price_in_million_per_square_meter'].str.replace(' đ/m²','').str.replace('.','').astype(float) * 0.000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummies for categorical columns\n",
    "dummy_type_of_housing = pd.get_dummies(df_renamed.type_of_housing, prefix=\"housing_type\")\n",
    "dummy_legal_paper = pd.get_dummies(df_renamed.legal_paper, prefix=\"legal_paper\")\n",
    "dummy_district = pd.get_dummies(df_renamed.district, prefix=\"district\")\n",
    "dummy_ward = pd.get_dummies(df_renamed.ward, prefix=\"ward\")\n",
    "\n",
    "df_cleaned = pd.concat([df_renamed, dummy_type_of_housing, dummy_legal_paper, dummy_district, dummy_ward], axis=1)\n",
    "df_cleaned = df_cleaned.drop(['index', 'date', 'address', 'district', 'ward', 'type_of_housing', 'legal_paper'], axis = 1)\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing outliers using IQR method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outlier_IQR(df, series):\n",
    "    Q1=df[series].quantile(0.25)\n",
    "    Q3=df[series].quantile(0.75)\n",
    "    IQR=Q3-Q1\n",
    "    df_final=df[~((df[series]<(Q1-1.5*IQR)) | (df[series]>(Q3+1.5*IQR)))]\n",
    "    return df_final\n",
    "\n",
    "removed_outliers = df_cleaned\n",
    "columns_to_remove_outliers = ['num_floors', 'num_bed_rooms', 'squared_meter_area', 'length_meter',\n",
    "                              'width_meter', 'price_in_million_per_square_meter']\n",
    "for column in columns_to_remove_outliers:\n",
    "    removed_outliers = remove_outlier_IQR(removed_outliers, column)\n",
    "    \n",
    "print(\"The final length of the dataset is\", str(len(removed_outliers)), \"rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Consider the distribution of housing in districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_renamed['district'].value_counts().keys()\n",
    "\n",
    "plt.barh(x[:10], df_renamed['district'].value_counts()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==> There is an increasing trend in the city center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Distribution of housing types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_renamed['type_of_housing'].value_counts()\n",
    "\n",
    "plt.barh(x.keys(), x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==> In the center, there is not much land, so alley houses are more prominent than other types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. The legal documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_renamed['legal_paper'].value_counts()\n",
    "plt.barh(x.keys(), x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Distribution of number of floors, number of bedrooms, area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_renamed['num_floors'].value_counts()\n",
    "plt.bar(x.keys()[:10], x[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==> Mainly houses with 3 to 6 floors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_renamed['num_bed_rooms'].value_counts()\n",
    "plt.bar(x.keys(), x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==> Mainly houses have with 3 to 4 rooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_renamed['squared_meter_area']\n",
    "plt.hist(x, edgecolor='black', color='red', bins=np.arange(0, 150+1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The area distribution is mainly around 30 -> 60 m2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training a Artificial Neural Network (ANN) for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale variables using Standard Scaler and create train-test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = removed_outliers\n",
    "\n",
    "# Separate predictors and response (price) variables\n",
    "X = housing.loc[:, housing.columns != 'price_in_million_per_square_meter']\n",
    "y = housing[['price_in_million_per_square_meter']]\n",
    "to_be_scaled = ['num_floors', 'num_bed_rooms', 'squared_meter_area', 'length_meter', 'width_meter']\n",
    "\n",
    "# Initiate scaler\n",
    "PredictorScaler=StandardScaler()\n",
    "TargetVarScaler=StandardScaler()\n",
    "\n",
    "X_scaled = X\n",
    "y_scaled = y\n",
    "\n",
    "# Storing the fit object for reference and reverse the scaling later\n",
    "PredictorScalerFit=PredictorScaler.fit(X_scaled[to_be_scaled])\n",
    "TargetVarScalerFit=TargetVarScaler.fit(y_scaled)\n",
    " \n",
    "# Generating the standardized values of X and y\n",
    "X_scaled[to_be_scaled]=PredictorScalerFit.transform(X_scaled[to_be_scaled])\n",
    "y_scaled=TargetVarScalerFit.transform(y)\n",
    "\n",
    "X_array = np.array(X_scaled.values).astype(\"float32\")\n",
    "y_array = np.array(y_scaled).astype(\"float32\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_array, y_array, test_size=0.2, random_state=2032)\n",
    "\n",
    "# Sanity check to see if all train and test arrays have correct dimensions\n",
    "if X_train.shape[0] == y_train.shape[0] and X_train.shape[1] == X_test.shape[1] and X_test.shape[0] == y_test.shape[0] and y_train.shape[1] == y_test.shape[1]:\n",
    "    print(\"All train and test sets have correct dimensions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the ANN to the Training set\n",
    "ANN = Sequential()                \n",
    "ANN.add(Dense(units=10, input_dim=X_train.shape[1], \n",
    "                kernel_initializer='normal', activation='relu'))\n",
    "ANN.add(Dense(1, kernel_initializer='normal'))\n",
    "ANN.compile(loss='mean_squared_error', optimizer='adam')\n",
    "ANN.fit(X_train, y_train,batch_size = 30,\n",
    "        epochs = 10, verbose=0)\n",
    "\n",
    "# Generating Predictions on testing data\n",
    "ANN_predictions = ANN.predict(X_test)\n",
    " \n",
    "# Scaling the predicted Price data back to original price scale\n",
    "ANN_predictions = TargetVarScalerFit.inverse_transform(ANN_predictions)\n",
    " \n",
    "# Scaling the y_test Price data back to original price scale\n",
    "y_test_orig = TargetVarScalerFit.inverse_transform(y_test)\n",
    " \n",
    "# Scaling the test data back to original scale\n",
    "Test_Data = np.concatenate((PredictorScalerFit.inverse_transform(X_test[:,:5]), X_test[:,5:]), axis=1)\n",
    "\n",
    "# Recreating the dataset, now with predicted price using the ANN model\n",
    "TestingData = pd.DataFrame(data=Test_Data, columns=X.columns)\n",
    "TestingData['Price'] = y_test_orig\n",
    "TestingData['ANN_predictions'] = ANN_predictions\n",
    "\n",
    "TestingData[['Price', 'ANN_predictions']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding best parameters for the ANN using grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off TensorFlow messages and warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"KMP_SETTINGS\"] = \"false\"\n",
    "\n",
    "# Create the base model\n",
    "def create_regression_ANN(optimizer_trial='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=10, input_dim=X_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer_trial)\n",
    "    return model\n",
    "\n",
    "# Creathe a dictionary for trial parameters\n",
    "ANN_params = {'batch_size':[10, 20, 30, 50],\n",
    "             'epochs':[10, 20, 50],\n",
    "             'model__optimizer_trial':['adam', 'rmsprop']}\n",
    "\n",
    "ANN_trial = KerasRegressor(model=create_regression_ANN, verbose=0)\n",
    "\n",
    "# Initiate the grid search and storing best parameters for later reference\n",
    "ANN_grid_search = GridSearchCV(estimator=ANN_trial, param_grid=ANN_params, \n",
    "                               cv=3, n_jobs = -1).fit(X_train, y_train, verbose=0)\n",
    "ANN_best_params = ANN_grid_search.best_params_\n",
    "\n",
    "# Showing the best parameters\n",
    "ANN_best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the ANN model with the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the ANN to the Training set\n",
    "ANN = Sequential()                \n",
    "ANN.add(Dense(units=10, input_dim=X_train.shape[1], \n",
    "                kernel_initializer='normal', activation='relu'))\n",
    "ANN.add(Dense(1, kernel_initializer='normal'))\n",
    "ANN.compile(loss='mean_squared_error', optimizer=ANN_best_params['model__optimizer_trial'])\n",
    "ANN.fit(X_train, y_train,batch_size = int(ANN_best_params['batch_size']),\n",
    "        epochs = int(ANN_best_params['epochs']), verbose=0)\n",
    "\n",
    "# Generating Predictions on testing data\n",
    "ANN_predictions = ANN.predict(X_test)\n",
    " \n",
    "# Scaling the predicted Price data back to original price scale\n",
    "ANN_predictions = TargetVarScalerFit.inverse_transform(ANN_predictions)\n",
    " \n",
    "# Scaling the y_test Price data back to original price scale\n",
    "y_test_orig = TargetVarScalerFit.inverse_transform(y_test)\n",
    " \n",
    "# Scaling the test data back to original scale\n",
    "Test_Data = np.concatenate((PredictorScalerFit.inverse_transform(X_test[:,:5]), X_test[:,5:]), axis=1)\n",
    "\n",
    "# Recreating the dataset, now with predicted price using the ANN model\n",
    "TestingData = pd.DataFrame(data=Test_Data, columns=X.columns)\n",
    "TestingData['Price'] = y_test_orig\n",
    "TestingData['ANN_predictions'] = ANN_predictions\n",
    "\n",
    "TestingData[['Price', 'ANN_predictions']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training a Random Forest model for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding best parameters for the Random Forest model using random search and grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of random parameters for the model\n",
    "RF_random_grid = {'n_estimators': [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)],\n",
    "               'max_features': ['auto', 'sqrt', 'log2'],\n",
    "               'max_depth': [int(x) for x in np.linspace(10, 100, num = 10)],\n",
    "               'min_samples_split': [2, 5, 10],\n",
    "               'min_samples_leaf': [1, 2, 4],\n",
    "               'bootstrap': [True, False]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off TensorFlow messages and warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"KMP_SETTINGS\"] = \"false\"\n",
    "\n",
    "# Create the base RF model and fit the random search\n",
    "RF_regressor = RandomForestRegressor()\n",
    "RF_random_search = RandomizedSearchCV(estimator=RF_regressor, param_distributions=RF_random_grid, n_iter=100, cv=5, \n",
    "                                      verbose=0, random_state=2022, n_jobs = -1).fit(X_train, np.ravel(y_train))\n",
    "RF_best_params = RF_random_search.best_params_\n",
    "RF_best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Narrowing the parameters grid based on the best parameters given by the random search, then feed the grid to a grid search\n",
    "RF_param_grid = {'n_estimators': [RF_best_params['n_estimators']-100, RF_best_params['n_estimators'], RF_best_params['n_estimators']+100],\n",
    "               'max_features': ['sqrt', 'log2'],\n",
    "               'max_depth': [RF_best_params['max_depth'] - 10, RF_best_params['max_depth'], RF_best_params['max_depth']+10],\n",
    "               'min_samples_split': [5, 10],\n",
    "               'min_samples_leaf': [1, 2],\n",
    "               'bootstrap': [True, False]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off TensorFlow messages and warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"KMP_SETTINGS\"] = \"false\"\n",
    "\n",
    "# Create another base RF model and fit the grid search\n",
    "RF_regressor_2 = RandomForestRegressor()\n",
    "RF_grid_search = GridSearchCV(estimator=RF_regressor_2, param_grid=RF_param_grid, \n",
    "                              cv=3, n_jobs=-1, verbose=0).fit(X_train, np.ravel(y_train))\n",
    "\n",
    "# Showing the best parameters\n",
    "RF_grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the RF model with the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a RF model with the best parameters\n",
    "RF = RF_grid_search.best_estimator_\n",
    "\n",
    "# Generating Predictions on testing data\n",
    "RF_predictions = RF.predict(X_test)\n",
    "\n",
    "# Reshaping the predictions to 2D for inverse transform\n",
    "RF_predictions_reshaped = RF_predictions.reshape(-1, 1)\n",
    "\n",
    "# Scaling the predicted Price data back to original price scale\n",
    "RF_predictions_original_scale = TargetVarScalerFit.inverse_transform(RF_predictions_reshaped)\n",
    "\n",
    "# Add predictions back to TestingData\n",
    "TestingData['RF_predictions'] = RF_predictions_original_scale.flatten()\n",
    "\n",
    "# Display the results\n",
    "print(TestingData[['Price', 'ANN_predictions', 'RF_predictions']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training a KNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=5)\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the KNN model with the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11, 13, 15],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski'],\n",
    "    'p': [1, 2] \n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(KNeighborsRegressor(), param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_k = grid_search.best_params_['n_neighbors']\n",
    "print(\"Optimal number of neighbors:\", best_k)\n",
    "\n",
    "knn_optimized = KNeighborsRegressor(n_neighbors=best_k)\n",
    "knn_optimized.fit(X_train, y_train)\n",
    "y_pred_optimized = knn_optimized.predict(X_test)\n",
    "\n",
    "mse_optimized = mean_squared_error(y_test, y_pred_optimized)\n",
    "print(\"Optimized Mean Squared Error:\", mse_optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(y_test, y_pred_optimized)\n",
    "plt.xlabel(\"Actual Prices\")\n",
    "plt.ylabel(\"Predicted Prices\")\n",
    "plt.title(\"Actual vs Predicted Prices (KNN)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training a Decision Tree Regressor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Decision Tree Regressor\n",
    "tree_model = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = tree_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"R^2 Score: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    \"max_depth\": [3, 5, 10, None],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"max_features\": [None, \"sqrt\", \"log2\"]\n",
    "}\n",
    "\n",
    "# Use GridSearchCV for tuning\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=DecisionTreeRegressor(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    cv=5,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train with grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Best model\n",
    "best_tree_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the tuned model\n",
    "y_pred_tuned = best_tree_model.predict(X_test)\n",
    "mse_tuned = mean_squared_error(y_test, y_pred_tuned)\n",
    "r2_tuned = r2_score(y_test, y_pred_tuned)\n",
    "\n",
    "print(f\"Tuned Mean Squared Error (MSE): {mse_tuned}\")\n",
    "print(f\"Tuned R^2 Score: {r2_tuned}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final result: Artifical Neural Network vs. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function evaluate the predictions\n",
    "def Accuracy_Score(orig, pred):\n",
    "    MAPE = np.mean(100 * (np.abs(orig - pred) / orig))\n",
    "    return(100-MAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing scores for both the ANN and the RF model\n",
    "print(\"Accuracy for the ANN model is:\", str(Accuracy_Score(TestingData['Price'], TestingData['ANN_predictions'])))\n",
    "print(\"Accuracy for the RF model is:\", str(Accuracy_Score(TestingData['Price'], TestingData['RF_predictions'])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
